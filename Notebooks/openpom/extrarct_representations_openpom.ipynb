{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Example script for training MPNN-POM model"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T05:31:58.052357Z",
     "start_time": "2024-07-03T05:31:58.046147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T05:31:58.796555Z",
     "start_time": "2024-07-03T05:31:58.792301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parent_dir = \"/Midgard/home/farzantn/phd/Olfaction/MoLFormer_N2024\"\n",
    "sys.path.append(parent_dir)\n",
    "parent_dir=\"/Midgard/home/farzantn/mambaforge/envs/MolTran_CUDA11_cuda/lib/python3.8\"\n",
    "sys.path.append(parent_dir)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-07-03T05:31:59.837514Z",
     "start_time": "2024-07-03T05:31:59.585431Z"
    }
   },
   "source": [
    "import deepchem as dc\n",
    "import os\n",
    "os.environ['TF_ENABLE_MLIR_OPTIMIZATIONS'] = '1'\n",
    "from openpom.feat.graph_featurizer import GraphFeaturizer, GraphConvConstants\n",
    "from openpom.utils.data_utils import get_class_imbalance_ratio\n",
    "from openpom.models.mpnn_pom import MPNNPOMModel\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils.util_alignment import set_seeds\n",
    "import pandas as pd\n",
    "from constants import *"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fast_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StandardScaler\n\u001B[0;32m---> 14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil_alignment\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m set_seeds\n",
      "File \u001B[0;32m~/phd/Olfaction/MoLFormer_N2024/utils/util_alignment.py:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfast_transformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmasking\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LengthMask \u001B[38;5;28;01mas\u001B[39;00m LM\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpairwise\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cosine_similarity\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'fast_transformers'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T05:32:09.557046Z",
     "start_time": "2024-07-03T05:32:09.527940Z"
    }
   },
   "source": [
    "seed = 2024\n",
    "set_seeds(2024)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'set_seeds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m seed \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2024\u001B[39m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mset_seeds\u001B[49m(\u001B[38;5;241m2024\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'set_seeds' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": "",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "def convert_todf_openpom(embeddings_dataset,cids,subjects=None,y=None):\n",
    "    embeddings_dataset = pd.DataFrame(embeddings_dataset)\n",
    "    embeddings_dataset['embeddings'] = embeddings_dataset.loc[:, 0:768].values.tolist()\n",
    "    embeddings_dataset['CID'] = cids\n",
    "    if subjects is not None:\n",
    "        embeddings_dataset['subject'] = subjects\n",
    "    if y is not None:\n",
    "        y_dataset = pd.DataFrame(y)\n",
    "        y_dataset['y'] = y_dataset.loc[:, 0:256].values.tolist()\n",
    "    \n",
    "        df = pd.concat([embeddings_dataset, y_dataset], axis=1)\n",
    "        return df\n",
    "    else:\n",
    "        return embeddings_dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "\n",
    "def embed_mols(input_file):\n",
    "    # get dataset\n",
    "    # print(os.getcwd())\n",
    "    featurizer = GraphFeaturizer()\n",
    "    smiles_field = 'nonStereoSMILES'\n",
    "    loader = dc.data.CSVLoader(tasks=[],\n",
    "                       feature_field=smiles_field,\n",
    "                       featurizer=featurizer)\n",
    "    dataset = loader.create_dataset(inputs=[input_file])\n",
    "    \n",
    "    embeddings=model.predict_embedding(dataset)\n",
    "    return embeddings,dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "def postproce_molembeddings(embeddings,index):\n",
    "    # molecules_embeddings_penultimate = torch.cat(embeddings)\n",
    "    df_molecules_embeddings = pd.DataFrame(embeddings, index=index)\n",
    "    df_molecules_embeddings['Combined'] = df_molecules_embeddings.loc[:, '0':'767'].values.tolist()\n",
    "    df_molecules_embeddings=df_molecules_embeddings.reset_index()\n",
    "    return(df_molecules_embeddings)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "source": [
    "def prepare_mols_helper(input_file,tasks,mol_type=\"nonStereoSMILES\",index=\"cid\"):\n",
    "    featurizer = GraphFeaturizer()\n",
    "    # smiles_field = 'nonStereoSMILES'\n",
    "    loader = dc.data.CSVLoader(tasks=tasks,\n",
    "                   feature_field=mol_type,\n",
    "                   featurizer=featurizer\n",
    "                          )\n",
    "    dataset = loader.create_dataset(inputs=[input_file])\n",
    "    df_mols = pd.read_csv(input_file)\n",
    "    print(df_mols.columns)\n",
    "\n",
    "    df_mols_embeddings_original=model.predict_embedding(dataset)\n",
    "    return df_mols_embeddings_original,dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# download curated dataset\n",
    "# !wget https://raw.githubusercontent.com/ARY2260/openpom/main/openpom/data/curated_datasets/curated_GS_LF_merged_4983.csv\n",
    "\n",
    "# The curated dataset can also found at `openpom/data/curated_datasets/curated_GS_LF_merged_4983.csv` in the repo.\n",
    "\n",
    "input_file = '/local_storage/datasets/farzaneh/openpom/data/curated_datasets/curated_GS_LF_merged_4983.csv' # or new downloaded file path"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "df_gslf = pd.read_csv(input_file)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# get dataset\n",
    "print(os.getcwd())\n",
    "featurizer = GraphFeaturizer()\n",
    "smiles_field = 'nonStereoSMILES'\n",
    "loader = dc.data.CSVLoader(tasks=gs_lf_tasks,\n",
    "                   feature_field=smiles_field,\n",
    "                   featurizer=featurizer)\n",
    "dataset = loader.create_dataset(inputs=[input_file])\n",
    "n_tasks = len(dataset.tasks)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "source": [
    "# def pom_frame(pom_embeds, y, dir,required_desc ):\n",
    "#     # pom_embeds = model.predict_embedding(dataset)\n",
    "#     # y_preds = model.predict(dataset)\n",
    "#     # required_desc = list(dataset.tasks)\n",
    "#     type1 = {'floral': '#F3F1F7', 'subs': {'muguet': '#FAD7E6', 'lavender': '#8883BE', 'jasmin': '#BD81B7'}}\n",
    "#     type2 = {'meaty': '#F5EBE8', 'subs': {'savory': '#FBB360', 'beefy': '#7B382A', 'roasted': '#F7A69E'}}\n",
    "#     type3 = {'ethereal': '#F2F6EC', 'subs': {'cognac': '#BCE2D2', 'fermented': '#79944F', 'alcoholic': '#C2DA8F'}}\n",
    "#         \n",
    "#     # Assuming you have your features in the 'features' array\n",
    "#     pca = PCA(n_components=2, iterated_power=10)  # You can choose the number of components you want (e.g., 2 for 2D visualization)\n",
    "#     reduced_features = pca.fit_transform(pom_embeds) # try different variations\n",
    "# \n",
    "#     variance_explained = pca.explained_variance_ratio_\n",
    "# \n",
    "#     # Variance explained by PC1 and PC2\n",
    "#     variance_pc1 = variance_explained[0]\n",
    "#     variance_pc2 = variance_explained[1]\n",
    "# \n",
    "#     # if is_preds:\n",
    "#     #     y = np.where(y_preds>threshold, 1.0, 0.0) # try quartile range (or rank)\n",
    "#     # else:\n",
    "#     #     y = dataset.y\n",
    "# \n",
    "#     # Generate grid points to evaluate the KDE on (try kernel convolution)\n",
    "#     x_grid, y_grid = np.meshgrid(np.linspace(reduced_features[:, 0].min(), reduced_features[:, 0].max(), 500),\n",
    "#                                  np.linspace(reduced_features[:, 1].min(), reduced_features[:, 1].max(), 500))\n",
    "#     grid_points = np.vstack([x_grid.ravel(), y_grid.ravel()])\n",
    "#     def get_kde_values(label):\n",
    "#         plot_idx = required_desc.index(label)\n",
    "#         # print(y[:, plot_idx])\n",
    "#         label_indices = np.where(y[:, plot_idx] == 1)[0]\n",
    "#         kde_label = gaussian_kde(reduced_features[label_indices].T)\n",
    "#         kde_values_label = kde_label(grid_points)\n",
    "#         kde_values_label = kde_values_label.reshape(x_grid.shape)\n",
    "#         return kde_values_label\n",
    "#     \n",
    "#     def plot_contours(type_dictionary, bbox_to_anchor):\n",
    "#         main_label = list(type_dictionary.keys())[0]\n",
    "#         plt.contourf(x_grid, y_grid, get_kde_values(main_label), levels=1, colors=['#00000000',type_dictionary[main_label],type_dictionary[main_label]])\n",
    "#         legend_elements = []\n",
    "#         for label, color in type_dictionary['subs'].items():\n",
    "#             plt.contour(x_grid, y_grid, get_kde_values(label), levels=1, colors=color, linewidths=2)\n",
    "#             legend_elements.append(Patch(facecolor=color, label=label))\n",
    "#         legend = plt.legend(handles=legend_elements, title=main_label, bbox_to_anchor=bbox_to_anchor)\n",
    "#         legend.get_frame().set_facecolor(type_dictionary[main_label])\n",
    "#         plt.gca().add_artist(legend)\n",
    "# \n",
    "#     plt.figure(figsize=(15, 10))\n",
    "#     plt.title('KDE Density Estimation with Contours in Reduced Space')\n",
    "#     plt.xlabel(f'Principal Component 1 ({round(variance_pc1*100, ndigits=2)}%)')\n",
    "#     plt.ylabel(f'Principal Component 2 ({round(variance_pc2*100, ndigits=2)}%)')\n",
    "#     plot_contours(type_dictionary=type1, bbox_to_anchor = (0.2, 0.8))\n",
    "#     plot_contours(type_dictionary=type2, bbox_to_anchor = (0.9, 0.4))\n",
    "#     plot_contours(type_dictionary=type3, bbox_to_anchor = (0.3, 0.1))\n",
    "#     # plt.colorbar(label='Density')\n",
    "#     # plt.show()\n",
    "#     # png_file = os.path.join(dir, 'pom_frame.png')\n",
    "#     # plt.savefig(png_file)\n",
    "#     plt.savefig(\"figs/realign_islands.png\")\n",
    "#     plt.show()\n",
    "#     plt.close()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# get train valid test splits\n",
    "\n",
    "randomstratifiedsplitter = dc.splits.RandomStratifiedSplitter()\n",
    "train_dataset, valid_dataset, test_dataset = randomstratifiedsplitter.train_valid_test_split(dataset, frac_train = 0.8, frac_valid = 0.1, frac_test = 0.1, seed = seed)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "train,valid,test=randomstratifiedsplitter.split(dataset, frac_train = 0.8, frac_valid = 0.1, frac_test = 0.1, seed = seed)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "df_train_valid_test = pd.DataFrame({'main_idx': train + valid + test,\n",
    "                   'split': ['train'] * len(train) + ['valid'] * len(valid) + ['test'] * len(test)})\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "df_train_valid_test.to_csv(\"gslf-splits-idx-third.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "# with open('train_test_split', 'w') as f:\n",
    "     \n",
    "#     # using csv.writer method from CSV package\n",
    "#     write = csv.writer(train)\n",
    "#     write = csv.writer(test)\n",
    "#     write = csv.writer(split)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "for i in range(len(train)):\n",
    "    if not np.array_equal(train_dataset.y[i],dataset.y[train[i]]):\n",
    "        print(i)\n",
    "\n",
    "for i in range(len(valid)):\n",
    "    if not np.array_equal(valid_dataset.y[i],dataset.y[valid[i]]):\n",
    "        print(i)\n",
    "\n",
    "for i in range(len(test)):\n",
    "    if not np.array_equal(test_dataset.y[i],dataset.y[test[i]]):\n",
    "        print(i)\n",
    "\n",
    "\n",
    "for i in range(len(train)):\n",
    "    if not np.array_equal(train_dataset.y[i],df_gslf.iloc[train[i]].values[2:].tolist()):\n",
    "        print(i)\n",
    "\n",
    "for i in range(len(valid)):\n",
    "    if not np.array_equal(valid_dataset.y[i],df_gslf.iloc[valid[i]].values[2:].tolist()):\n",
    "        print(i)\n",
    "\n",
    "for i in range(len(test)):\n",
    "    if not np.array_equal(test_dataset.y[i],df_gslf.iloc[test[i]].values[2:].tolist()):\n",
    "        print(i)\n",
    "        "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "train_ratios = get_class_imbalance_ratio(train_dataset)\n",
    "assert len(train_ratios) == n_tasks"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "train_dataset.y"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# learning_rate = ExponentialDecay(initial_rate=0.001, decay_rate=0.5, decay_steps=32*15, staircase=True)\n",
    "learning_rate = 0.001"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# initialize model\n",
    "device_name = 'cuda'\n",
    "model = MPNNPOMModel(n_tasks = n_tasks,\n",
    "                            batch_size=128,\n",
    "                            learning_rate=learning_rate,\n",
    "                            class_imbalance_ratio = train_ratios,\n",
    "                            loss_aggr_type = 'sum',\n",
    "                            node_out_feats = 100,\n",
    "                            edge_hidden_feats = 75,\n",
    "                            edge_out_feats = 100,\n",
    "                            num_step_message_passing = 5,\n",
    "                            mpnn_residual = True,\n",
    "                            message_aggregator_type = 'sum',\n",
    "                            mode = 'classification',\n",
    "                            number_atom_features = GraphConvConstants.ATOM_FDIM,\n",
    "                            number_bond_features = GraphConvConstants.BOND_FDIM,\n",
    "                            n_classes = 1,\n",
    "                            readout_type = 'set2set',\n",
    "                            num_step_set2set = 3,\n",
    "                            num_layer_set2set = 2,\n",
    "                            ffn_hidden_list= [392, 392],\n",
    "                            ffn_embeddings = 256,\n",
    "                            ffn_activation = 'relu',\n",
    "                            ffn_dropout_p = 0.12,\n",
    "                            ffn_dropout_at_input_no_act = False,\n",
    "                            weight_decay = 1e-5,\n",
    "                            self_loop = False,\n",
    "                            optimizer_name = 'adam',\n",
    "                            log_frequency = 32,\n",
    "                            model_dir = './examples/experiments',\n",
    "                            device_name=device_name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "nb_epoch = 150"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "model.model_dir"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# start_time = datetime.now()\n",
    "# loss_train =[]\n",
    "# for epoch in range(1, nb_epoch+1):\n",
    "#         loss = model.fit(\n",
    "#               train_dataset,\n",
    "#               nb_epoch=1,\n",
    "#               max_checkpoints_to_keep=1,\n",
    "#               deterministic=False,\n",
    "#               restore=epoch>1,all_losses=loss_train)\n",
    "#         train_scores = model.evaluate(train_dataset, [metric])['roc_auc_score']\n",
    "#         valid_scores = model.evaluate(valid_dataset, [metric])['roc_auc_score']\n",
    "#         # loss_train.append(loss)\n",
    "#         print(f\"epoch {epoch}/{nb_epoch} ; loss = {loss}; train_scores = {train_scores}; valid_scores = {valid_scores}\")\n",
    "# # model.save_checkpoint()\n",
    "# end_time = datetime.now()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "model.load_from_pretrained(model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "test_scores = model.evaluate(test_dataset, [metric])['roc_auc_score']\n",
    "# print(\"time_taken: \", str(end_time-start_time))\n",
    "print(\"test_score: \", test_scores)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GS-LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "source": [
    "embeddings_dataset=model.predict_embedding(dataset)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "source": [
    "cids_gslf= df_gslf.index.values.tolist()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "source": [
    "df_embeddings = convert_todf_openpom(embeddings_dataset,cids_gslf,None,dataset.y)\n",
    "df_embeddings.to_csv('gslf_pom_embeddings_Apr17.csv', index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "source": [
    "# df_embeddings17.head(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "source": [
    "\n",
    "input_file_sagar= '/local_storage/datasets/farzaneh/openpom/data/curated_datasets/curated_sagar_subjects_nonaminus.csv'\n",
    "df_sagar_temp=pd.read_csv(input_file_sagar)\n",
    "cids_sagar= df_sagar_temp['cid'].values.tolist()\n",
    "subjects_sagar= df_sagar_temp['subject'].values.tolist()\n",
    "sagar_tasks= df_sagar_temp.columns.to_list()[1:16]\n",
    "df_mols_embeddings_original,sagar_dataset=prepare_mols_helper(input_file_sagar,sagar_tasks)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "source": [
    "df_sagar_temp"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "source": [
    "df_embeddings_sagar = convert_todf_openpom(df_mols_embeddings_original,cids_sagar,subjects_sagar,sagar_dataset.y)\n",
    "df_embeddings_sagar.to_csv('sagar_pom_embeddings_Apr17.csv', index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "source": [
    "input_file_keller= '/local_storage/datasets/farzaneh/openpom/data/curated_datasets/curated_keller2016_nona.csv'\n",
    "df_keller_temp=pd.read_csv(input_file_keller)\n",
    "keller_tasks= df_keller_temp.columns.to_list()[5:]\n",
    "cids_keller= df_keller_temp['CID'].values.tolist()\n",
    "subjects_keller= df_keller_temp['Subject'].values.tolist()\n",
    "df_mols_embeddings_original_keller,keller_dataset=prepare_mols_helper(input_file_keller,keller_tasks,index=\"CID\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "source": [
    "df_embeddings_keller = convert_todf_openpom(df_mols_embeddings_original_keller,cids_keller,subjects_keller,keller_dataset.y)\n",
    "df_embeddings_keller.to_csv('keller_pom_embeddings_Apr17.csv', index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ravia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "source": [
    "input_file_ravia = '/local_storage/datasets/farzaneh/openpom/data/curated_datasets/ravia_molecules.csv'\n",
    "df_ravia_temp=pd.read_csv(input_file_ravia)\n",
    "embeddings_ravia,dataset=embed_mols(input_file_ravia)\n",
    "cids_ravia= df_ravia_temp['CID'].values.tolist()\n",
    "df_embeddings_ravia = convert_todf_openpom(embeddings_ravia,cids_ravia)\n",
    "df_embeddings_ravia.to_csv('ravia_pom_embeddings_Apr17.csv', index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "source": [
    "input_file_snitz = '/local_storage/datasets/farzaneh/openpom/data/curated_datasets/snitz_molecules.csv'\n",
    "df_snitz_temp=pd.read_csv(input_file_snitz)\n",
    "embeddings_snitz,dataset=embed_mols(input_file_snitz)\n",
    "cids_snitz= df_snitz_temp['CID'].values.tolist()\n",
    "df_embeddings_snitz = convert_todf_openpom(embeddings_snitz,cids_snitz)\n",
    "df_embeddings_snitz.to_csv('snitz_pom_embeddings_Apr17.csv', index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
